{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a20ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation for LSTM FCN for Time Series Classification\n",
    "# Original code in TensorFlow https://github.com/titu1994/LSTM-FCN\n",
    "# Paper https://arxiv.org/abs/1709.05206\n",
    "#\n",
    "# By David Campos and Teodor Vernica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011c86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sail.models.torch.lstm_fcn import _LSTM_FCN, LSTM_FCN_Classifier\n",
    "from sail.models.torch.fcn import FCN_Classifier # An optional model without LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aade475",
   "metadata": {},
   "source": [
    "1. Importing and checking that the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32738ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3852, 0.3504, 0.2645],\n",
      "        [0.3343, 0.4296, 0.2361],\n",
      "        [0.3402, 0.3755, 0.2843],\n",
      "        [0.3702, 0.3729, 0.2570],\n",
      "        [0.3791, 0.3489, 0.2720]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Model works\n",
    "import torch\n",
    "input = torch.randn(5, 10)\n",
    "\n",
    "model = _LSTM_FCN(in_channels=1,input_size=input.size()[1],classes=3)\n",
    "output = model(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ab038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0889\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0842\u001b[0m  0.1159\n",
      "<class 'sail.models.torch.lstm_fcn.LSTM_FCN_Classifier'>[initialized](\n",
      "  module_=_LSTM_FCN(\n",
      "    (lstm): LSTM(1, 128, num_layers=8)\n",
      "    (drop): Dropout(p=0.8, inplace=False)\n",
      "    (conv_layers): Sequential(\n",
      "      (0): ConvBlock(\n",
      "        (conv_layers): Sequential(\n",
      "          (0): Conv1dSamePadding(1, 128, kernel_size=(8,), stride=(1,))\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): ConvBlock(\n",
      "        (conv_layers): Sequential(\n",
      "          (0): Conv1dSamePadding(128, 256, kernel_size=(5,), stride=(1,))\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): ConvBlock(\n",
      "        (conv_layers): Sequential(\n",
      "          (0): Conv1dSamePadding(256, 128, kernel_size=(3,), stride=(1,))\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  ),\n",
      ")\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Skorch works\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X = torch.randn(5, 10)\n",
    "y = np.random.randint(3, size=10)\n",
    "\n",
    "X, y = make_classification(30, 10, n_informative=5, random_state=0)\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=10, lstm_layers=8, classes=3)\n",
    "\n",
    "partial_fit = model_skorch.partial_fit(X,y)\n",
    "print(partial_fit)\n",
    "predict = model_skorch.predict(X)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d398e54",
   "metadata": {},
   "source": [
    "2. Loading a time-series dataset [(ACSF1)](http://timeseriesclassification.com/description.php?Dataset=ACSF1), from [timeseriesclassification.com](http://timeseriesclassification.com/dataset.php) to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdbf8069",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Tests/Datasets/ACSF1/ACSF1_TRAIN.arff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40220\\1817778939.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marff\u001b[0m \u001b[1;31m# pip install liac-arff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Tests/Datasets/ACSF1/ACSF1_TRAIN.arff'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Tests/Datasets/ACSF1/ACSF1_TRAIN.arff'"
     ]
    }
   ],
   "source": [
    "import arff # pip install liac-arff\n",
    "\n",
    "train_dataset = arff.load(open('./Tests/Datasets/ACSF1/ACSF1_TRAIN.arff'))\n",
    "train_data = np.array(train_dataset['data'])\n",
    "\n",
    "X_train = train_data[:,0:-1]\n",
    "y_train = train_data[:,-1]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "test_dataset = arff.load(open('./Tests/Datasets/ACSF1/ACSF1_TEST.arff'))\n",
    "\n",
    "test_data = np.array(test_dataset['data'])\n",
    "\n",
    "X_test = test_data[:,0:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c438a51",
   "metadata": {},
   "source": [
    "3. **Batch training.** Testing the model on the time-series data with batch training. The model learns, given the entire data-set and enough epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8adb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.3179\u001b[0m       \u001b[32m0.1000\u001b[0m        \u001b[35m2.3031\u001b[0m  9.9225\n",
      "      2        \u001b[36m2.2976\u001b[0m       0.1000        \u001b[35m2.3026\u001b[0m  10.0113\n",
      "      3        \u001b[36m2.2778\u001b[0m       0.1000        \u001b[35m2.3020\u001b[0m  10.1257\n",
      "      4        \u001b[36m2.2620\u001b[0m       0.1000        \u001b[35m2.3012\u001b[0m  10.0037\n",
      "      5        \u001b[36m2.2460\u001b[0m       0.1000        \u001b[35m2.3006\u001b[0m  10.6474\n",
      "      6        \u001b[36m2.2273\u001b[0m       0.1000        \u001b[35m2.2995\u001b[0m  10.4894\n",
      "      7        \u001b[36m2.2127\u001b[0m       0.1000        \u001b[35m2.2982\u001b[0m  11.0346\n",
      "      8        \u001b[36m2.2001\u001b[0m       0.1000        \u001b[35m2.2968\u001b[0m  11.9972\n",
      "      9        \u001b[36m2.1863\u001b[0m       0.1000        \u001b[35m2.2951\u001b[0m  10.7660\n",
      "     10        \u001b[36m2.1680\u001b[0m       0.1000        \u001b[35m2.2931\u001b[0m  10.3665\n",
      "     11        \u001b[36m2.1606\u001b[0m       0.1000        \u001b[35m2.2908\u001b[0m  11.6405\n",
      "     12        \u001b[36m2.1494\u001b[0m       0.1000        \u001b[35m2.2880\u001b[0m  10.2885\n",
      "     13        \u001b[36m2.1399\u001b[0m       0.1000        \u001b[35m2.2849\u001b[0m  10.2103\n",
      "     14        \u001b[36m2.1273\u001b[0m       0.1000        \u001b[35m2.2815\u001b[0m  10.5141\n",
      "     15        \u001b[36m2.1082\u001b[0m       0.1000        \u001b[35m2.2776\u001b[0m  10.2455\n",
      "     16        \u001b[36m2.1058\u001b[0m       \u001b[32m0.1500\u001b[0m        \u001b[35m2.2733\u001b[0m  10.3770\n",
      "     17        \u001b[36m2.0885\u001b[0m       0.1500        \u001b[35m2.2685\u001b[0m  10.1778\n",
      "     18        \u001b[36m2.0802\u001b[0m       \u001b[32m0.2000\u001b[0m        \u001b[35m2.2632\u001b[0m  10.2460\n",
      "     19        \u001b[36m2.0663\u001b[0m       0.2000        \u001b[35m2.2574\u001b[0m  10.7557\n",
      "     20        \u001b[36m2.0570\u001b[0m       0.2000        \u001b[35m2.2510\u001b[0m  10.2258\n",
      "     21        \u001b[36m2.0445\u001b[0m       0.2000        \u001b[35m2.2443\u001b[0m  10.3488\n",
      "     22        \u001b[36m2.0416\u001b[0m       0.2000        \u001b[35m2.2369\u001b[0m  10.6310\n",
      "     23        \u001b[36m2.0255\u001b[0m       0.2000        \u001b[35m2.2292\u001b[0m  10.2530\n",
      "     24        \u001b[36m2.0132\u001b[0m       0.2000        \u001b[35m2.2206\u001b[0m  10.3382\n",
      "     25        \u001b[36m2.0089\u001b[0m       \u001b[32m0.2500\u001b[0m        \u001b[35m2.2124\u001b[0m  10.4399\n",
      "0.27\n",
      "[7 7 7 7 7 7 7 7 7 7 7 8 7 7 7 7 7 7 7 7 1 4 1 1 4 4 4 4 4 4 7 7 0 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 7 7 7 4 7 7 7 7 7 1 7 1 7 7 1 7 7 8 8\n",
      " 8 8 8 7 7 8 7 7 7 7 7 7 7 7 7 7 7 7 4 4 1 7 1 1 4 4]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Test on time series with all data at once\n",
    "classes = 10\n",
    "\n",
    "model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "#model_skorch = FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "\n",
    "#good results around 50 epochs\n",
    "for i in range(0,25):\n",
    "    partial_fit = model_skorch.partial_fit(X_train, y_train)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9df94f",
   "metadata": {},
   "source": [
    "4. **Mini-batch training.** In an online environment, we might not have access to all data at once or might not afford to re-train the model with all data for multiple epochs. So we test the model with mini-batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75bed8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.7193\u001b[0m       \u001b[32m0.0000\u001b[0m        \u001b[35m2.3046\u001b[0m  0.9520\n",
      "      2        \u001b[36m2.3143\u001b[0m       0.0000        \u001b[35m2.2700\u001b[0m  1.1025\n",
      "      3        2.3743       \u001b[32m1.0000\u001b[0m        \u001b[35m2.2255\u001b[0m  0.9215\n",
      "      4        2.4565       0.0000        2.3542  0.9896\n",
      "      5        2.3683       0.0000        2.2514  1.0583\n",
      "      6        \u001b[36m2.2524\u001b[0m       0.0000        2.3163  0.9432\n",
      "      7        2.4469       0.0000        2.2880  0.9584\n",
      "      8        2.2599       1.0000        \u001b[35m2.1995\u001b[0m  0.9693\n",
      "      9        2.4866       0.0000        2.2820  0.9783\n",
      "     10        2.4897       0.0000        2.3153  1.0398\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# Test on time series data in mini-batches\n",
    "from sklearn.utils import gen_batches\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "\n",
    "# We can not use epochs because it is online learning\n",
    "# for i in range(0,10): \n",
    "#     partial_fit = model_skorch.partial_fit(X_train, y_train)\n",
    "\n",
    "# Batch processing, we have 100 time series samples, so the model trains with 10 examples every time\n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f4058",
   "metadata": {},
   "source": [
    "5. **Mini-batch training without LSTM.** The model does not do as well in an on-line setting. That could be attributed to the LSTM component requiring more training, which depends on the batch. To compare, we test a version of the model without the LSTM component on the same dataset dataset, which is faster and sometimes gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3734d833",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.1796\u001b[0m       \u001b[32m0.0000\u001b[0m        \u001b[35m2.2876\u001b[0m  0.1515\n",
      "      2        2.4042       \u001b[32m1.0000\u001b[0m        \u001b[35m2.2230\u001b[0m  0.1652\n",
      "      3        2.4136       0.0000        2.2803  0.1677\n",
      "      4        2.4996       0.0000        2.3319  0.1640\n",
      "      5        2.7003       0.0000        2.3119  0.1372\n",
      "      6        \u001b[36m2.1575\u001b[0m       0.0000        2.2390  0.1572\n",
      "      7        2.6755       0.0000        2.3708  0.1844\n",
      "      8        2.3463       1.0000        \u001b[35m2.2202\u001b[0m  0.1749\n",
      "      9        2.5307       0.0000        2.2383  0.1622\n",
      "     10        2.3080       0.0000        2.2919  0.1383\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 5 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n",
      "0.11\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "model_skorch = FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "    \n",
    "# Batch processing, we have 100 time series samples, so the model trains with 10 examples every time\n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43969c36",
   "metadata": {},
   "source": [
    "6. **Loading a larger dataset.** To test this more, we can try the two incremental versions of the model on a larger time-series dataset, such as [FordA](http://timeseriesclassification.com/description.php?Dataset=FordA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc165467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3601, 500)\n",
      "(3601,)\n",
      "[[-0.79717165 -0.66439205 -0.37301463 ... -0.66439205 -1.0737958\n",
      "  -1.5643427 ]\n",
      " [ 0.8048547   0.6346286   0.37347448 ... -0.71488506 -0.5604429\n",
      "  -0.31908643]\n",
      " [ 0.7279851   0.11128392 -0.49912438 ...  0.39446303  0.3394004\n",
      "   0.2553906 ]\n",
      " ...\n",
      " [-0.5700543  -0.33316523 -0.29351854 ... -1.3937145  -0.9427333\n",
      "  -0.27072167]\n",
      " [ 2.006732    2.07915     2.0220363  ... -0.43214503 -0.44123125\n",
      "  -0.2807089 ]\n",
      " [-0.1252409  -0.32536268 -0.48823696 ...  0.5557605   0.574451\n",
      "   0.573116  ]]\n",
      "[0 1 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = arff.load(open('./Tests/Datasets/FordA/FordA_TRAIN.arff'))\n",
    "train_data = np.array(train_dataset['data'])\n",
    "\n",
    "X_train = train_data[:,0:-1]\n",
    "y_train = train_data[:,-1]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_train)\n",
    "        \n",
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "        \n",
    "print(y_train)\n",
    "    \n",
    "\n",
    "test_dataset = arff.load(open('./Tests/Datasets/FordA/FordA_TEST.arff'))\n",
    "\n",
    "test_data = np.array(test_dataset['data'])\n",
    "\n",
    "X_test = test_data[:,0:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "y_test = np.where(y_test == -1, 0, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95367912",
   "metadata": {},
   "source": [
    "7. **Mini-batch learning on the larger dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3088a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6982\u001b[0m       \u001b[32m0.5500\u001b[0m        \u001b[35m0.6924\u001b[0m  4.4546\n",
      "      2        \u001b[36m0.6889\u001b[0m       0.5500        0.6929  3.8726\n",
      "      3        0.6917       0.5000        \u001b[35m0.6922\u001b[0m  3.9328\n",
      "      4        0.6917       0.5000        0.6931  3.7746\n",
      "      5        \u001b[36m0.6751\u001b[0m       0.5000        \u001b[35m0.6911\u001b[0m  3.7757\n",
      "      6        0.6799       0.5500        0.6915  3.7699\n",
      "      7        0.6829       0.4500        0.6934  3.8896\n",
      "      8        0.6826       0.5500        0.6922  3.7851\n",
      "      9        0.6768       0.4500        0.6944  3.8494\n",
      "     10        \u001b[36m0.6645\u001b[0m       0.5000        0.6927  3.7519\n",
      "     11        0.6667       0.5500        \u001b[35m0.6888\u001b[0m  3.8673\n",
      "     12        \u001b[36m0.6641\u001b[0m       0.4500        0.6931  3.8072\n",
      "     13        \u001b[36m0.6578\u001b[0m       0.5000        0.6920  3.8309\n",
      "     14        0.6604       \u001b[32m0.7000\u001b[0m        \u001b[35m0.6879\u001b[0m  3.9680\n",
      "     15        0.6650       0.6500        0.6879  3.9321\n",
      "     16        \u001b[36m0.6428\u001b[0m       0.6000        0.6889  3.9903\n",
      "     17        0.6569       0.6000        0.6886  4.0574\n",
      "     18        0.6439       0.6500        0.6881  3.9486\n",
      "     19        0.6483       0.5500        0.6916  3.8442\n",
      "     20        0.6735       0.6500        0.6883  3.9383\n",
      "     21        0.6704       0.6500        \u001b[35m0.6813\u001b[0m  4.0651\n",
      "     22        0.6616       \u001b[32m0.7500\u001b[0m        0.6822  3.8532\n",
      "     23        \u001b[36m0.6415\u001b[0m       0.7500        \u001b[35m0.6767\u001b[0m  3.8368\n",
      "     24        \u001b[36m0.6142\u001b[0m       \u001b[32m0.8500\u001b[0m        \u001b[35m0.6664\u001b[0m  3.8085\n",
      "     25        0.6215       0.7000        0.6839  3.8649\n",
      "     26        0.6218       0.7500        0.6741  3.8753\n",
      "     27        0.6418       0.6500        0.6772  3.8483\n",
      "     28        0.6589       0.7500        \u001b[35m0.6585\u001b[0m  3.8630\n",
      "     29        0.6427       0.7500        \u001b[35m0.6568\u001b[0m  3.8026\n",
      "     30        0.6514       0.5000        0.6977  3.7908\n",
      "     31        0.6270       0.7000        0.6648  3.9360\n",
      "     32        0.6379       0.8000        \u001b[35m0.6564\u001b[0m  3.9102\n",
      "     33        0.6408       0.8000        \u001b[35m0.6400\u001b[0m  3.9928\n",
      "     34        0.6286       0.6500        0.6703  3.9705\n",
      "     35        0.6437       0.8500        \u001b[35m0.6349\u001b[0m  3.9335\n",
      "     36        0.6275       0.6667        0.6459  3.8688\n",
      "0.696969696969697\n",
      "[0 0 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import gen_batches\n",
    "\n",
    "batch_size = 100\n",
    "classes = 2\n",
    "\n",
    "model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=500, lstm_layers=8, classes=classes)\n",
    "\n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    y_train_batch = np.where(y_train_batch == -1, 0, y_train_batch)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8c284",
   "metadata": {},
   "source": [
    "8. **Mini-batch learning on the larger dataset without LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe611ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7348\u001b[0m       \u001b[32m0.5500\u001b[0m        \u001b[35m0.6886\u001b[0m  0.5065\n",
      "      2        0.7622       0.5500        0.6889  0.5163\n",
      "      3        \u001b[36m0.7322\u001b[0m       0.5000        0.6937  0.5396\n",
      "      4        0.7387       0.5000        0.6937  0.5183\n",
      "      5        \u001b[36m0.6921\u001b[0m       0.5000        0.6931  0.4586\n",
      "      6        \u001b[36m0.6592\u001b[0m       0.5500        \u001b[35m0.6885\u001b[0m  0.5328\n",
      "      7        0.7031       0.4500        0.6951  0.4973\n",
      "      8        0.6777       0.5500        0.6896  0.5201\n",
      "      9        0.7038       0.4000        0.6950  0.4917\n",
      "     10        \u001b[36m0.6569\u001b[0m       0.4500        0.6913  0.5304\n",
      "     11        \u001b[36m0.6344\u001b[0m       \u001b[32m0.6500\u001b[0m        \u001b[35m0.6873\u001b[0m  0.5186\n",
      "     12        0.6473       \u001b[32m0.7000\u001b[0m        0.6892  0.5178\n",
      "     13        \u001b[36m0.6092\u001b[0m       0.6000        0.6883  0.4884\n",
      "     14        0.6390       \u001b[32m0.7500\u001b[0m        \u001b[35m0.6824\u001b[0m  0.5065\n",
      "     15        0.6266       0.6000        0.6877  0.5480\n",
      "     16        \u001b[36m0.5928\u001b[0m       0.5500        0.6908  0.4963\n",
      "     17        0.6121       0.6000        0.6889  0.5193\n",
      "     18        0.6022       0.5500        0.6869  0.4785\n",
      "     19        0.6031       0.5500        0.6918  0.5292\n",
      "     20        0.6446       0.6000        \u001b[35m0.6795\u001b[0m  0.4874\n",
      "     21        0.6413       0.6000        \u001b[35m0.6738\u001b[0m  0.5459\n",
      "     22        0.6168       0.6000        0.6768  0.4712\n",
      "     23        0.5952       0.6500        \u001b[35m0.6672\u001b[0m  0.5589\n",
      "     24        \u001b[36m0.5579\u001b[0m       0.7500        \u001b[35m0.6491\u001b[0m  0.4864\n",
      "     25        0.5767       0.5000        0.6875  0.5407\n",
      "     26        0.5648       0.7500        0.6557  0.4969\n",
      "     27        0.6001       0.6500        0.6683  0.5028\n",
      "     28        0.6216       0.7500        \u001b[35m0.6338\u001b[0m  0.5100\n",
      "     29        0.5943       0.7500        \u001b[35m0.6332\u001b[0m  0.4929\n",
      "     30        0.6166       0.5000        0.6995  0.5195\n",
      "     31        0.5812       0.6000        0.6551  0.4998\n",
      "     32        0.5911       0.7000        \u001b[35m0.6319\u001b[0m  0.5212\n",
      "     33        0.5993       \u001b[32m0.8000\u001b[0m        \u001b[35m0.6000\u001b[0m  0.4964\n",
      "     34        0.5769       0.6500        0.6442  0.5563\n",
      "     35        0.5954       0.8000        \u001b[35m0.5940\u001b[0m  0.4986\n",
      "     36        0.5837       0.6667        0.6301  0.5327\n",
      "[0 0 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.706060606060606\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "classes = 2\n",
    "\n",
    "#model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "model_skorch = FCN_Classifier(in_channels=1,input_size=945, lstm_layers=8, classes=classes)\n",
    "    \n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    y_train_batch = np.where(y_train_batch == -1, 0, y_train_batch)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929f06d",
   "metadata": {},
   "source": [
    "9. Both models perform better on the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729f028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
