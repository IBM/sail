{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series classification\n",
    "\n",
    "Implementation of Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline (2016, arXiv) in PyTorch by using a skorch wrapper\n",
    "In this script, we are using a dataset that was originally used in the paper\n",
    "\n",
    "Authors: Marina Georgati, Hao Miao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sail.models.torch.conv_net import ConvNet\n",
    "from sail.models.torch import dataset\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Adiac data:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Adiac data: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = ['Adiac']\n",
    "\"\"\"\n",
    "Download the time series datasets used in the paper\n",
    "from http://www.timeseriesclassification.com\n",
    "NonInvThorax1 and NonInvThorax2 are missing\n",
    "\"\"\"\n",
    "datasets_pbar = tqdm(datasets)\n",
    "print(datasets_pbar)\n",
    "for dataset in datasets_pbar:\n",
    "    datasets_pbar.set_description(\"Extracting {0} data\".format(dataset))\n",
    "    zipurl = f\"http://www.timeseriesclassification.com/aeon-toolkit/Adiac.zip\"\n",
    "    with urlopen(zipurl) as zipfile:\n",
    "        with ZipFile(BytesIO(zipfile.read())) as zfile:\n",
    "            zfile.extractall(f\"data/\")\n",
    "\n",
    "    assert os.path.exists(f\"data/{dataset}_TRAIN.arff\"), dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Adiac: 100%|██████████| 1/1 [00:00<00:00,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# A dictionary is created by train/test DataLoaders for the downloaded dataset\n",
    "dataset_dictionary = dataset.data_dictionary(\"data\", datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training dataset is: (390, 176)\n",
      "The shape of the test dataset is: (391, 176)\n",
      "Number of input units: 176\n",
      "Number of classes: 37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5515\u001b[0m  0.7129\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      2        \u001b[36m3.2614\u001b[0m  0.7283\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      3        \u001b[36m3.1515\u001b[0m  0.6837\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      4        \u001b[36m3.0357\u001b[0m  0.7082\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      5        \u001b[36m2.8706\u001b[0m  0.7037\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      6        \u001b[36m2.7199\u001b[0m  0.6931\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      7        \u001b[36m2.5973\u001b[0m  0.7120\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      8        \u001b[36m2.4902\u001b[0m  0.7006\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      9        \u001b[36m2.4036\u001b[0m  0.7135\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "     10        \u001b[36m2.3128\u001b[0m  0.6890\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Score: 0.22506393861892582\n"
     ]
    }
   ],
   "source": [
    "for dataset, dataloader in dataset_dictionary.items():\n",
    "    # For using the downloaded dataset, a dataset and a dataloader are created. \n",
    "    # Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples\n",
    "    print('The shape of the training dataset is:', dataloader['train'].dataset.x.shape)\n",
    "    print('The shape of the test dataset is:', dataloader['test'].dataset.x.shape)\n",
    "    \n",
    "    time_steps = dataloader['test'].dataset.x.shape[-1] # The Number of input units\n",
    "    n_classes  = len(np.unique(dataloader['test'].dataset.y)) # Number of classes\n",
    "    print('Number of input units:', time_steps)\n",
    "    print('Number of classes:', n_classes)\n",
    "    \n",
    "    # The Neural Net is initialized with fixed hyperparameters\n",
    "    nn = ConvNet(time_steps, n_classes)\n",
    "\n",
    "    # The training set is defined here, modification of the datatype might be required\n",
    "    y_train = dataloader['train'].dataset.y\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "    \n",
    "    # The testing set is defined here, modification of the datatype might be required\n",
    "    y_test = dataloader['test'].dataset.y\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "    # Fit the model on the training dataset \n",
    "    nn.fit(dataloader['train'].dataset.x, y_train)\n",
    "    # Return probability estimates for samples\n",
    "    nn.predict_proba(dataloader['test'].dataset.x)\n",
    "    # Return class labels for samples in X\n",
    "    nn.predict(dataloader['test'].dataset.x)\n",
    "    # Print score\n",
    "    print('Score:', nn.score(dataloader['test'].dataset.x, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390, 176)\n",
      "(391, 176)\n",
      "176 37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.5603\u001b[0m  0.7273\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      2        \u001b[36m3.2648\u001b[0m  0.7182\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      3        \u001b[36m3.1521\u001b[0m  0.6899\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      4        \u001b[36m3.0392\u001b[0m  0.7205\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      5        \u001b[36m2.8902\u001b[0m  0.6912\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      6        \u001b[36m2.7307\u001b[0m  0.7402\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      7        \u001b[36m2.6005\u001b[0m  0.6922\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      8        \u001b[36m2.4725\u001b[0m  0.7128\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      9        \u001b[36m2.3983\u001b[0m  0.7198\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "     10        \u001b[36m2.3070\u001b[0m  0.6988\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.29411764705882354\n"
     ]
    }
   ],
   "source": [
    "# OR DIFFERENTLY , calling the wrapper from src.tsc\n",
    "for dataset, dataloader in dataset_dictionary.items():\n",
    "    print(dataloader['train'].dataset.x.shape)\n",
    "    print(dataloader['test'].dataset.x.shape)\n",
    "    \n",
    "    time_steps = dataloader['test'].dataset.x.shape[-1]\n",
    "    n_classes  = len(np.unique(dataloader['test'].dataset.y))\n",
    "    print(time_steps, n_classes)\n",
    "\n",
    "    # The Neural Net is initialized with fixed hyperparameters\n",
    "    nn = ConvNet(time_steps, n_classes)\n",
    "\n",
    "    y_train = dataloader['train'].dataset.y\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "    \n",
    "    y_test = dataloader['test'].dataset.y\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "    nn.fit(dataloader['train'].dataset.x, y_train)\n",
    "    print(nn.score(dataloader['test'].dataset.x, y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d4520545ca233a18fcbfaf6de76cf00aab1835d3dba167ce44a459e1af2c853"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('imla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
