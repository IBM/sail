{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series classification\n",
    "Implementation of Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline (2016, arXiv) in PyTorch by using a skorch wrapper \n",
    "In this script, we are using a dataset that was originally used in the paper\n",
    "\n",
    "Authors: Marina Georgati, Hao Miao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our modules\n",
    "import numpy as np\n",
    "from sail.models.torch.fcn_classification import _ConvNetModel, ConvNet\n",
    "from sail.models.torch import utils\n",
    "# Import SKORCH NN classifier\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Adiac:   0%|          | 0/1 [00:00<?, ?it/s]  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100 1991k  100 1991k    0     0  6256k      0 --:--:-- --:--:-- --:--:-- 6383k\n",
      "Extracting Adiac data: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/Adiac.zip\n",
      "  inflating: data/Adiac.txt          \n",
      "  inflating: data/Adiac_TEST.arff    \n",
      "  inflating: data/Adiac_TEST.txt     \n",
      "  inflating: data/Adiac_TRAIN.arff   \n",
      "  inflating: data/Adiac_TRAIN.txt    \n",
      "  inflating: data/Adiac_TEST.ts      \n",
      "  inflating: data/Adiac_TRAIN.ts     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The dataset is downloaded by http://www.timeseriesclassification.com/Downloads/\n",
    "datasets = ['Adiac']\n",
    "utils.download_datasets(datasets)  # uncomment this to download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Adiac:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Current working directory1: /Users/dhaval/Projects/MORE/sail-version-bump/examples/models/torch\n",
      "Current working directory1: /Users/dhaval/Projects/MORE/sail-version-bump/examples/models/torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Adiac: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# A dictionary is created by train/test DataLoaders for the downloaded dataset\n",
    "dataset_dictionary = utils.data_dictionary(\"data\", datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training dataset is: (390, 176)\n",
      "The shape of the test dataset is: (391, 176)\n",
      "Number of input units: 176\n",
      "Number of classes: 37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhaval/.pyenv/versions/3.10.9/envs/venv-sail/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m4.1648\u001b[0m       \u001b[32m0.0769\u001b[0m        \u001b[35m3.8666\u001b[0m  1.1127\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      2        \u001b[36m3.4044\u001b[0m       \u001b[32m0.1026\u001b[0m        \u001b[35m3.2742\u001b[0m  1.0177\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      3        \u001b[36m3.3437\u001b[0m       0.1026        3.2897  0.9572\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      4        \u001b[36m3.3110\u001b[0m       0.0769        3.3009  1.0149\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      5        \u001b[36m3.2947\u001b[0m       0.0641        3.3013  0.8869\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      6        \u001b[36m3.2871\u001b[0m       0.1026        \u001b[35m3.1933\u001b[0m  0.8998\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      7        \u001b[36m3.2589\u001b[0m       0.0641        \u001b[35m3.1671\u001b[0m  0.8621\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      8        \u001b[36m3.2433\u001b[0m       0.0769        3.2267  0.8924\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      9        \u001b[36m3.2134\u001b[0m       0.0897        3.4716  0.9388\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "     10        \u001b[36m3.1745\u001b[0m       0.0897        3.3314  1.0490\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Score: 0.07161125319693094\n"
     ]
    }
   ],
   "source": [
    "for dataset, dataloader in dataset_dictionary.items():\n",
    "    # For using the downloaded dataset, a dataset and a dataloader are created. \n",
    "    # Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples\n",
    "    print('The shape of the training dataset is:', dataloader['train'].dataset.x.shape)\n",
    "    print('The shape of the test dataset is:', dataloader['test'].dataset.x.shape)\n",
    "    \n",
    "    time_steps = dataloader['test'].dataset.x.shape[-1] # The Number of input units\n",
    "    n_classes  = len(np.unique(dataloader['test'].dataset.y)) # Number of classes\n",
    "    print('Number of input units:', time_steps)\n",
    "    print('Number of classes:', n_classes)\n",
    "    \n",
    "    # The Neural Net is initialized with fixed hyperparameters\n",
    "    nn = NeuralNetClassifier(\n",
    "        _ConvNetModel(time_steps, n_classes), \n",
    "        max_epochs=10, \n",
    "        lr=0.1, \n",
    "        batch_size=12, \n",
    "        optimizer=torch.optim.Adam,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        # Shuffle training data on each epoch\n",
    "        iterator_train__shuffle=False )\n",
    "\n",
    "    # The training set is defined here, modification of the datatype might be required\n",
    "    y_train = dataloader['train'].dataset.y\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "    \n",
    "    # The testing set is defined here, modification of the datatype might be required\n",
    "    y_test = dataloader['test'].dataset.y\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "    # Fit the model on the training dataset \n",
    "    nn.fit(dataloader['train'].dataset.x, y_train)\n",
    "    # Return probability estimates for samples\n",
    "    nn.predict_proba(dataloader['test'].dataset.x)\n",
    "    # Return class labels for samples in X\n",
    "    nn.predict(dataloader['test'].dataset.x)\n",
    "    # Print score\n",
    "    print('Score:', nn.score(dataloader['test'].dataset.x, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390, 176)\n",
      "(391, 176)\n",
      "176 37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhaval/.pyenv/versions/3.10.9/envs/venv-sail/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m3.6742\u001b[0m       \u001b[32m0.1026\u001b[0m        \u001b[35m3.3186\u001b[0m  1.0046\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      2        \u001b[36m3.2847\u001b[0m       0.0769        \u001b[35m3.1678\u001b[0m  1.0733\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      3        \u001b[36m3.1685\u001b[0m       0.1026        \u001b[35m3.1043\u001b[0m  0.8554\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      4        \u001b[36m3.0932\u001b[0m       0.1026        \u001b[35m3.0013\u001b[0m  0.8970\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      5        \u001b[36m3.0068\u001b[0m       \u001b[32m0.1667\u001b[0m        \u001b[35m2.8364\u001b[0m  0.8827\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      6        \u001b[36m2.8917\u001b[0m       \u001b[32m0.2179\u001b[0m        \u001b[35m2.6710\u001b[0m  1.0015\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      7        \u001b[36m2.7415\u001b[0m       \u001b[32m0.2308\u001b[0m        \u001b[35m2.5764\u001b[0m  0.8859\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      8        \u001b[36m2.6257\u001b[0m       0.1923        \u001b[35m2.5754\u001b[0m  0.9333\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      9        \u001b[36m2.4902\u001b[0m       0.2179        \u001b[35m2.5752\u001b[0m  0.9293\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "     10        \u001b[36m2.3663\u001b[0m       \u001b[32m0.3333\u001b[0m        \u001b[35m2.1997\u001b[0m  1.0010\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.27621483375959077\n"
     ]
    }
   ],
   "source": [
    "# OR DIFFERENTLY , calling the wrapper from src.tsc\n",
    "for dataset, dataloader in dataset_dictionary.items():\n",
    "    print(dataloader['train'].dataset.x.shape)\n",
    "    print(dataloader['test'].dataset.x.shape)\n",
    "    \n",
    "    time_steps = dataloader['test'].dataset.x.shape[-1]\n",
    "    n_classes  = len(np.unique(dataloader['test'].dataset.y))\n",
    "    print(time_steps, n_classes)\n",
    "\n",
    "    # The Neural Net is initialized with fixed hyperparameters\n",
    "    nn = ConvNet(time_steps, n_classes)\n",
    "\n",
    "    y_train = dataloader['train'].dataset.y\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "    \n",
    "    y_test = dataloader['test'].dataset.y\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "    nn.fit(dataloader['train'].dataset.x, y_train)\n",
    "    print(nn.score(dataloader['test'].dataset.x, y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d4520545ca233a18fcbfaf6de76cf00aab1835d3dba167ce44a459e1af2c853"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('imla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
