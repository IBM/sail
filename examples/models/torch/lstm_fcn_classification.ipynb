{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a20ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation for LSTM FCN for Time Series Classification\n",
    "# Original code in TensorFlow https://github.com/titu1994/LSTM-FCN\n",
    "# Paper https://arxiv.org/abs/1709.05206\n",
    "#\n",
    "# By David Campos and Teodor Vernica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011c86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sail.models.torch.lstm_fcn import _LSTM_FCN, LSTMFCNClassifier\n",
    "from sail.models.torch.fcn import FCNClassifier # An optional model without LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aade475",
   "metadata": {},
   "source": [
    "1. Importing and checking that the model works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32738ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3168, 0.3641, 0.3191],\n",
      "        [0.3346, 0.3581, 0.3073],\n",
      "        [0.3128, 0.3675, 0.3197],\n",
      "        [0.3079, 0.3701, 0.3220],\n",
      "        [0.3245, 0.3923, 0.2832]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Model works\n",
    "import torch\n",
    "input = torch.randn(5, 10)\n",
    "\n",
    "model = _LSTM_FCN(in_channels=1,input_size=input.size()[1],classes=3)\n",
    "output = model(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ab038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.1251\u001b[0m  0.0807\n",
      "<class 'sail.models.torch.lstm_fcn.LSTMFCNClassifier'>[initialized](\n",
      "  module_=_LSTM_FCN(\n",
      "    (lstm): LSTM(1, 128, num_layers=8)\n",
      "    (drop): Dropout(p=0.8, inplace=False)\n",
      "    (conv_layers): Sequential(\n",
      "      (0): ConvBlock(\n",
      "        (conv_layers): Sequential(\n",
      "          (0): Conv1dSamePadding(1, 128, kernel_size=(8,), stride=(1,))\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): ConvBlock(\n",
      "        (conv_layers): Sequential(\n",
      "          (0): Conv1dSamePadding(128, 256, kernel_size=(5,), stride=(1,))\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): ConvBlock(\n",
      "        (conv_layers): Sequential(\n",
      "          (0): Conv1dSamePadding(256, 128, kernel_size=(3,), stride=(1,))\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  ),\n",
      ")\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Skorch works\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X = torch.randn(5, 10)\n",
    "y = np.random.randint(3, size=10)\n",
    "\n",
    "X, y = make_classification(30, 10, n_informative=5, random_state=0)\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "model_skorch = LSTMFCNClassifier(in_channels=1,input_size=10, lstm_layers=8, classes=3)\n",
    "\n",
    "partial_fit = model_skorch.partial_fit(X,y)\n",
    "print(partial_fit)\n",
    "predict = model_skorch.predict(X)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d398e54",
   "metadata": {},
   "source": [
    "2. Loading a time-series dataset [(ACSF1)](http://timeseriesclassification.com/description.php?Dataset=ACSF1), from [timeseriesclassification.com](http://timeseriesclassification.com/dataset.php) to test the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fcc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "r = requests.get(\"https://www.timeseriesclassification.com/aeon-toolkit/ACSF1.zip\", stream=True)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbf8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.58475375 -0.58475375  1.730991   ... -0.5786034   1.7327257\n",
      "  -0.584734  ]\n",
      " [-0.59143436 -0.51110417  1.7268198  ... -0.5807305   1.7273961\n",
      "  -0.5807305 ]\n",
      " [-0.57794535 -0.57794535  1.7307931  ... -0.5497977   1.7347268\n",
      "  -0.5777511 ]\n",
      " ...\n",
      " [-0.99827707  0.10246194  1.6069248  ...  0.09938861  1.5636905\n",
      "  -0.69265294]\n",
      " [-0.9414731   0.58721364  1.5236441  ...  0.5822302   1.5482239\n",
      "  -0.645292  ]\n",
      " [-0.6615355  -0.6615355   1.5103272  ... -0.6605395   1.5101048\n",
      "  -0.6606845 ]]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import arff # pip install liac-arff\n",
    "\n",
    "train_dataset = arff.load(open('data/ACSF1_TRAIN.arff'))\n",
    "train_data = np.array(train_dataset['data'])\n",
    "\n",
    "X_train = train_data[:,0:-1]\n",
    "y_train = train_data[:,-1]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "test_dataset = arff.load(open('data/ACSF1_TEST.arff'))\n",
    "\n",
    "test_data = np.array(test_dataset['data'])\n",
    "\n",
    "X_test = test_data[:,0:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c438a51",
   "metadata": {},
   "source": [
    "3. **Batch training.** Testing the model on the time-series data with batch training. The model learns, given the entire data-set and enough epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8adb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.3072\u001b[0m  8.1256\n",
      "      2        \u001b[36m2.2861\u001b[0m  7.6160\n",
      "      3        \u001b[36m2.2775\u001b[0m  8.8214\n",
      "      4        \u001b[36m2.2580\u001b[0m  7.9017\n",
      "      5        \u001b[36m2.2459\u001b[0m  7.6660\n",
      "      6        \u001b[36m2.2294\u001b[0m  7.8081\n",
      "      7        \u001b[36m2.2166\u001b[0m  7.7658\n",
      "      8        \u001b[36m2.2022\u001b[0m  8.3024\n",
      "      9        \u001b[36m2.1983\u001b[0m  7.6986\n",
      "     10        \u001b[36m2.1846\u001b[0m  7.8461\n",
      "     11        \u001b[36m2.1674\u001b[0m  7.5518\n",
      "     12        \u001b[36m2.1591\u001b[0m  7.6780\n",
      "     13        \u001b[36m2.1426\u001b[0m  7.6557\n",
      "     14        \u001b[36m2.1387\u001b[0m  7.9912\n",
      "     15        \u001b[36m2.1215\u001b[0m  7.8065\n",
      "     16        \u001b[36m2.1110\u001b[0m  7.8733\n",
      "     17        \u001b[36m2.1008\u001b[0m  7.8367\n",
      "     18        \u001b[36m2.0901\u001b[0m  7.6144\n",
      "     19        \u001b[36m2.0835\u001b[0m  7.5784\n",
      "     20        \u001b[36m2.0687\u001b[0m  7.6918\n",
      "     21        \u001b[36m2.0625\u001b[0m  7.7997\n",
      "     22        \u001b[36m2.0463\u001b[0m  7.7066\n",
      "     23        \u001b[36m2.0401\u001b[0m  7.8978\n",
      "     24        \u001b[36m2.0259\u001b[0m  7.8032\n",
      "     25        \u001b[36m2.0177\u001b[0m  9.0410\n",
      "0.31\n",
      "[6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 6 3 3 2 4 6 6 4 4 4 4 4 4 0 6 0 6 0 6 6\n",
      " 6 0 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 6 6 6 4 6 6 6 6 6 2 6 2 6 6 6 3 3 3 3\n",
      " 3 3 3 6 3 3 6 6 6 3 6 6 6 6 6 6 6 6 4 4 4 6 4 4 4 4]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Test on time series with all data at once\n",
    "classes = 10\n",
    "\n",
    "model_skorch = LSTMFCNClassifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "#model_skorch = FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "\n",
    "#good results around 50 epochs\n",
    "for i in range(0,25):\n",
    "    partial_fit = model_skorch.partial_fit(X_train, y_train)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9df94f",
   "metadata": {},
   "source": [
    "4. **Mini-batch training.** In an online environment, we might not have access to all data at once or might not afford to re-train the model with all data for multiple epochs. So we test the model with mini-batch training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75bed8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.4948\u001b[0m  0.4546\n",
      "      2        \u001b[36m2.1583\u001b[0m  0.4182\n",
      "      3        2.3510  0.4466\n",
      "      4        2.2528  0.4571\n",
      "      5        2.7746  0.4494\n",
      "      6        2.2171  0.4411\n",
      "      7        2.5658  0.4806\n",
      "      8        2.5322  0.5316\n",
      "      9        2.3279  0.5038\n",
      "     10        2.5547  0.5076\n",
      "[7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# Test on time series data in mini-batches\n",
    "from sklearn.utils import gen_batches\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "model_skorch = LSTMFCNClassifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "\n",
    "# We can not use epochs because it is online learning\n",
    "# for i in range(0,10): \n",
    "#     partial_fit = model_skorch.partial_fit(X_train, y_train)\n",
    "\n",
    "# Batch processing, we have 100 time series samples, so the model trains with 10 examples every time\n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f4058",
   "metadata": {},
   "source": [
    "5. **Mini-batch training without LSTM.** The model does not do as well in an on-line setting. That could be attributed to the LSTM component requiring more training, which depends on the batch. To compare, we test a version of the model without the LSTM component on the same dataset dataset, which is faster and sometimes gives better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3734d833",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.7550\u001b[0m  0.0710\n",
      "      2        2.4336  0.0740\n",
      "      3        2.3812  0.0661\n",
      "      4        2.1999  0.0747\n",
      "      5        2.6929  0.0705\n",
      "      6        2.8383  0.0669\n",
      "      7        2.4946  0.0674\n",
      "      8        2.4134  0.0688\n",
      "      9        2.5974  0.0684\n",
      "     10        2.5182  0.0706\n",
      "[9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "model_skorch = FCNClassifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "    \n",
    "# Batch processing, we have 100 time series samples, so the model trains with 10 examples every time\n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43969c36",
   "metadata": {},
   "source": [
    "6. **Loading a larger dataset.** To test this more, we can try the two incremental versions of the model on a larger time-series dataset, such as [FordA](http://timeseriesclassification.com/description.php?Dataset=FordA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "594d98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "r = requests.get(\"https://www.timeseriesclassification.com/aeon-toolkit/FordA.zip\", stream=True)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc165467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3601, 500)\n",
      "(3601,)\n",
      "[[-0.79717165 -0.66439205 -0.37301463 ... -0.66439205 -1.0737958\n",
      "  -1.5643427 ]\n",
      " [ 0.8048547   0.6346286   0.37347448 ... -0.71488506 -0.5604429\n",
      "  -0.31908643]\n",
      " [ 0.7279851   0.11128392 -0.49912438 ...  0.39446303  0.3394004\n",
      "   0.2553906 ]\n",
      " ...\n",
      " [-0.5700543  -0.33316523 -0.29351854 ... -1.3937145  -0.9427333\n",
      "  -0.27072167]\n",
      " [ 2.006732    2.07915     2.0220363  ... -0.43214503 -0.44123125\n",
      "  -0.2807089 ]\n",
      " [-0.1252409  -0.32536268 -0.48823696 ...  0.5557605   0.574451\n",
      "   0.573116  ]]\n",
      "[0 1 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = arff.load(open('data/FordA_TRAIN.arff'))\n",
    "train_data = np.array(train_dataset['data'])\n",
    "\n",
    "X_train = train_data[:,0:-1]\n",
    "y_train = train_data[:,-1]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_train)\n",
    "        \n",
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "        \n",
    "print(y_train)\n",
    "    \n",
    "\n",
    "test_dataset = arff.load(open('data/FordA_TEST.arff'))\n",
    "\n",
    "test_data = np.array(test_dataset['data'])\n",
    "\n",
    "X_test = test_data[:,0:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "y_test = np.where(y_test == -1, 0, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95367912",
   "metadata": {},
   "source": [
    "7. **Mini-batch learning on the larger dataset.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3088a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6973\u001b[0m  3.1765\n",
      "      2        \u001b[36m0.6937\u001b[0m  3.1929\n",
      "      3        \u001b[36m0.6876\u001b[0m  3.1522\n",
      "      4        0.6907  3.2283\n",
      "      5        \u001b[36m0.6824\u001b[0m  3.0868\n",
      "      6        0.6851  3.1812\n",
      "      7        \u001b[36m0.6823\u001b[0m  3.1457\n",
      "      8        0.6830  3.2029\n",
      "      9        \u001b[36m0.6738\u001b[0m  3.2548\n",
      "     10        \u001b[36m0.6720\u001b[0m  3.1630\n",
      "     11        \u001b[36m0.6710\u001b[0m  3.0381\n",
      "     12        0.6731  2.9658\n",
      "     13        \u001b[36m0.6605\u001b[0m  3.3088\n",
      "     14        0.6660  3.3455\n",
      "     15        0.6712  3.3855\n",
      "     16        \u001b[36m0.6559\u001b[0m  3.4832\n",
      "     17        0.6622  3.4117\n",
      "     18        \u001b[36m0.6543\u001b[0m  3.5346\n",
      "     19        0.6566  3.6088\n",
      "     20        0.6742  3.4044\n",
      "     21        0.6660  3.4015\n",
      "     22        0.6586  3.3737\n",
      "     23        \u001b[36m0.6448\u001b[0m  3.3581\n",
      "     24        \u001b[36m0.6267\u001b[0m  3.3535\n",
      "     25        0.6335  3.1447\n",
      "     26        0.6350  3.2810\n",
      "     27        0.6480  3.4103\n",
      "     28        0.6518  3.2826\n",
      "     29        0.6367  3.2179\n",
      "     30        0.6591  3.3147\n",
      "     31        0.6312  3.3083\n",
      "     32        0.6311  3.3550\n",
      "     33        0.6369  3.4080\n",
      "     34        0.6317  3.2821\n",
      "     35        0.6316  3.3592\n",
      "     36        \u001b[36m0.6207\u001b[0m  3.5660\n",
      "[0 0 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.6848484848484848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import gen_batches\n",
    "\n",
    "batch_size = 100\n",
    "classes = 2\n",
    "\n",
    "model_skorch = LSTMFCNClassifier(in_channels=1,input_size=500, lstm_layers=8, classes=classes)\n",
    "\n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    y_train_batch = np.where(y_train_batch == -1, 0, y_train_batch)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8c284",
   "metadata": {},
   "source": [
    "8. **Mini-batch learning on the larger dataset without LSTM**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe611ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6997\u001b[0m  1.2719\n",
      "      2        \u001b[36m0.6901\u001b[0m  1.3205\n",
      "      3        \u001b[36m0.6806\u001b[0m  1.2954\n",
      "      4        0.6820  1.2806\n",
      "      5        \u001b[36m0.6652\u001b[0m  1.2817\n",
      "      6        \u001b[36m0.6650\u001b[0m  1.3706\n",
      "      7        \u001b[36m0.6642\u001b[0m  1.2863\n",
      "      8        0.6681  1.2711\n",
      "      9        \u001b[36m0.6496\u001b[0m  1.2817\n",
      "     10        \u001b[36m0.6426\u001b[0m  1.2918\n",
      "     11        \u001b[36m0.6389\u001b[0m  1.2511\n",
      "     12        0.6411  1.3365\n",
      "     13        \u001b[36m0.6243\u001b[0m  1.2790\n",
      "     14        0.6294  1.2918\n",
      "     15        0.6383  1.2567\n",
      "     16        \u001b[36m0.6139\u001b[0m  1.2814\n",
      "     17        0.6322  1.2965\n",
      "     18        0.6169  1.3592\n",
      "     19        0.6224  1.3820\n",
      "     20        0.6431  1.3096\n",
      "     21        0.6221  1.3296\n",
      "     22        0.6252  1.3688\n",
      "     23        \u001b[36m0.5974\u001b[0m  1.7388\n",
      "     24        \u001b[36m0.5647\u001b[0m  1.9660\n",
      "     25        0.5914  1.5536\n",
      "     26        0.5754  1.4937\n",
      "     27        0.6097  1.5677\n",
      "     28        0.6085  1.4862\n",
      "     29        0.5891  1.3916\n",
      "     30        0.6312  1.3925\n",
      "     31        0.5922  1.4482\n",
      "     32        0.5907  1.5169\n",
      "     33        0.5885  1.5616\n",
      "     34        0.5903  1.5911\n",
      "     35        0.5835  1.5792\n",
      "     36        0.5861  1.5097\n",
      "[0 0 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.6643939393939394\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "classes = 2\n",
    "\n",
    "#model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "model_skorch = FCNClassifier(in_channels=1,input_size=945, lstm_layers=8, classes=classes)\n",
    "    \n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    y_train_batch = np.where(y_train_batch == -1, 0, y_train_batch)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
