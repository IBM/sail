{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a20ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation for LSTM FCN for Time Series Classification\n",
    "# Original code in TensorFlow https://github.com/titu1994/LSTM-FCN\n",
    "# Paper https://arxiv.org/abs/1709.05206\n",
    "#\n",
    "# By David Campos and Teodor Vernica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011c86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sail.models.torch.lstm_fcn import _LSTM_FCN, LSTM_FCN_Classifier\n",
    "from sail.models.torch.fcn import FCN_Classifier # An optional model without LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aade475",
   "metadata": {},
   "source": [
    "1. Importing and checking that the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32738ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3576, 0.3481, 0.2942],\n",
      "        [0.3753, 0.3179, 0.3068],\n",
      "        [0.3621, 0.3147, 0.3232],\n",
      "        [0.3349, 0.3488, 0.3163],\n",
      "        [0.3572, 0.3358, 0.3070]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Model works\n",
    "import torch\n",
    "input = torch.randn(5, 10)\n",
    "\n",
    "model = _LSTM_FCN(in_channels=1,input_size=input.size()[1],classes=3)\n",
    "output = model(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ab038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1064\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0625\u001b[0m  0.1369\n",
      "<class 'sail.models.torch.lstm_fcn.LSTM_FCN_Classifier'>[initialized](\n",
      "  module_=_LSTM_FCN(\n",
      "    (lstm): LSTM(1, 128, num_layers=8)\n",
      "    (drop): Dropout(p=0.8, inplace=False)\n",
      "    (conv_layers): Sequential(\n",
      "      (0): ConvBlock(\n",
      "        (conv_layers): Sequential(\n",
      "          (0): Conv1dSamePadding(1, 128, kernel_size=(8,), stride=(1,))\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): ConvBlock(\n",
      "        (conv_layers): Sequential(\n",
      "          (0): Conv1dSamePadding(128, 256, kernel_size=(5,), stride=(1,))\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): ConvBlock(\n",
      "        (conv_layers): Sequential(\n",
      "          (0): Conv1dSamePadding(256, 128, kernel_size=(3,), stride=(1,))\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  ),\n",
      ")\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Skorch works\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X = torch.randn(5, 10)\n",
    "y = np.random.randint(3, size=10)\n",
    "\n",
    "X, y = make_classification(30, 10, n_informative=5, random_state=0)\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=10, lstm_layers=8, classes=3)\n",
    "\n",
    "partial_fit = model_skorch.partial_fit(X,y)\n",
    "print(partial_fit)\n",
    "predict = model_skorch.predict(X)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d398e54",
   "metadata": {},
   "source": [
    "2. Loading a time-series dataset [(ACSF1)](http://timeseriesclassification.com/description.php?Dataset=ACSF1), from [timeseriesclassification.com](http://timeseriesclassification.com/dataset.php) to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7fcc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "r = requests.get(\"http://timeseriesclassification.com/Downloads/ACSF1.zip\", stream=True)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdbf8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.58475375 -0.58475375  1.730991   ... -0.5786034   1.7327257\n",
      "  -0.584734  ]\n",
      " [-0.59143436 -0.51110417  1.7268198  ... -0.5807305   1.7273961\n",
      "  -0.5807305 ]\n",
      " [-0.57794535 -0.57794535  1.7307931  ... -0.5497977   1.7347268\n",
      "  -0.5777511 ]\n",
      " ...\n",
      " [-0.99827707  0.10246194  1.6069248  ...  0.09938861  1.5636905\n",
      "  -0.69265294]\n",
      " [-0.9414731   0.58721364  1.5236441  ...  0.5822302   1.5482239\n",
      "  -0.645292  ]\n",
      " [-0.6615355  -0.6615355   1.5103272  ... -0.6605395   1.5101048\n",
      "  -0.6606845 ]]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import arff # pip install liac-arff\n",
    "\n",
    "train_dataset = arff.load(open('data/ACSF1_TRAIN.arff'))\n",
    "train_data = np.array(train_dataset['data'])\n",
    "\n",
    "X_train = train_data[:,0:-1]\n",
    "y_train = train_data[:,-1]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "test_dataset = arff.load(open('data/ACSF1_TEST.arff'))\n",
    "\n",
    "test_data = np.array(test_dataset['data'])\n",
    "\n",
    "X_test = test_data[:,0:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c438a51",
   "metadata": {},
   "source": [
    "3. **Batch training.** Testing the model on the time-series data with batch training. The model learns, given the entire data-set and enough epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8adb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.3128\u001b[0m       \u001b[32m0.1000\u001b[0m        \u001b[35m2.3020\u001b[0m  5.9298\n",
      "      2        \u001b[36m2.2872\u001b[0m       0.1000        \u001b[35m2.3013\u001b[0m  5.8644\n",
      "      3        \u001b[36m2.2614\u001b[0m       0.1000        \u001b[35m2.3006\u001b[0m  6.9832\n",
      "      4        \u001b[36m2.2501\u001b[0m       0.1000        \u001b[35m2.2995\u001b[0m  6.5666\n",
      "      5        \u001b[36m2.2326\u001b[0m       0.1000        \u001b[35m2.2984\u001b[0m  5.9988\n",
      "      6        \u001b[36m2.2215\u001b[0m       0.1000        \u001b[35m2.2973\u001b[0m  6.9001\n",
      "      7        \u001b[36m2.2004\u001b[0m       0.1000        \u001b[35m2.2960\u001b[0m  6.4184\n",
      "      8        \u001b[36m2.1893\u001b[0m       0.1000        \u001b[35m2.2945\u001b[0m  6.0221\n",
      "      9        \u001b[36m2.1773\u001b[0m       0.1000        \u001b[35m2.2929\u001b[0m  5.9667\n",
      "     10        \u001b[36m2.1658\u001b[0m       0.1000        \u001b[35m2.2911\u001b[0m  6.5615\n",
      "     11        \u001b[36m2.1554\u001b[0m       0.1000        \u001b[35m2.2890\u001b[0m  6.6791\n",
      "     12        \u001b[36m2.1380\u001b[0m       \u001b[32m0.1500\u001b[0m        \u001b[35m2.2866\u001b[0m  6.4425\n",
      "     13        \u001b[36m2.1275\u001b[0m       0.1500        \u001b[35m2.2838\u001b[0m  6.1215\n",
      "     14        \u001b[36m2.1117\u001b[0m       \u001b[32m0.2000\u001b[0m        \u001b[35m2.2807\u001b[0m  5.6599\n",
      "     15        \u001b[36m2.1030\u001b[0m       0.2000        \u001b[35m2.2770\u001b[0m  5.7902\n",
      "     16        \u001b[36m2.0929\u001b[0m       \u001b[32m0.2500\u001b[0m        \u001b[35m2.2729\u001b[0m  5.8393\n",
      "     17        \u001b[36m2.0856\u001b[0m       0.2500        \u001b[35m2.2683\u001b[0m  5.6038\n",
      "     18        \u001b[36m2.0732\u001b[0m       0.2500        \u001b[35m2.2633\u001b[0m  5.7108\n",
      "     19        \u001b[36m2.0643\u001b[0m       0.2500        \u001b[35m2.2577\u001b[0m  5.6798\n",
      "     20        \u001b[36m2.0494\u001b[0m       0.2500        \u001b[35m2.2516\u001b[0m  5.5897\n",
      "     21        \u001b[36m2.0484\u001b[0m       \u001b[32m0.3000\u001b[0m        \u001b[35m2.2451\u001b[0m  6.5461\n",
      "     22        \u001b[36m2.0370\u001b[0m       \u001b[32m0.4000\u001b[0m        \u001b[35m2.2378\u001b[0m  6.4889\n",
      "     23        \u001b[36m2.0257\u001b[0m       0.4000        \u001b[35m2.2302\u001b[0m  5.8892\n",
      "     24        \u001b[36m2.0112\u001b[0m       0.4000        \u001b[35m2.2219\u001b[0m  5.7381\n",
      "     25        \u001b[36m2.0051\u001b[0m       0.4000        \u001b[35m2.2136\u001b[0m  5.4378\n",
      "0.4\n",
      "[6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 6 0 0 0 0 0\n",
      " 0 0 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 6 6 6 4 6 6 6 2 6 4 6 4 2 4 4 3 3 3 3\n",
      " 3 3 3 0 3 3 1 6 0 3 6 6 0 0 6 6 7 6 4 4 4 4 4 4 4 4]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Test on time series with all data at once\n",
    "classes = 10\n",
    "\n",
    "model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "#model_skorch = FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "\n",
    "#good results around 50 epochs\n",
    "for i in range(0,25):\n",
    "    partial_fit = model_skorch.partial_fit(X_train, y_train)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9df94f",
   "metadata": {},
   "source": [
    "4. **Mini-batch training.** In an online environment, we might not have access to all data at once or might not afford to re-train the model with all data for multiple epochs. So we test the model with mini-batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75bed8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.2672\u001b[0m       \u001b[32m0.0000\u001b[0m        \u001b[35m2.2882\u001b[0m  0.6118\n",
      "      2        \u001b[36m2.2516\u001b[0m       0.0000        \u001b[35m2.2663\u001b[0m  0.5760\n",
      "      3        2.6063       0.0000        2.3101  0.5379\n",
      "      4        2.2838       0.0000        \u001b[35m2.2313\u001b[0m  0.5332\n",
      "      5        2.2740       0.0000        \u001b[35m2.2259\u001b[0m  0.5363\n",
      "      6        2.4328       0.0000        2.3124  0.6128\n",
      "      7        2.5614       0.0000        2.3237  0.5423\n",
      "      8        2.3683       0.0000        \u001b[35m2.2130\u001b[0m  0.5439\n",
      "      9        2.7027       0.0000        2.3540  0.5344\n",
      "     10        2.4794       0.0000        2.3149  0.5381\n",
      "[8 8 8 8 8 8 8 8 8 8 8 6 8 6 6 6 6 8 6 6 8 8 8 8 8 8 8 8 6 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 6 8 8 8 6 8 8 8 8 8 8 8 8 8 8 8 6 6 6 6\n",
      " 6 6 6 8 6 6 8 8 8 6 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "# Test on time series data in mini-batches\n",
    "from sklearn.utils import gen_batches\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "\n",
    "# We can not use epochs because it is online learning\n",
    "# for i in range(0,10): \n",
    "#     partial_fit = model_skorch.partial_fit(X_train, y_train)\n",
    "\n",
    "# Batch processing, we have 100 time series samples, so the model trains with 10 examples every time\n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f4058",
   "metadata": {},
   "source": [
    "5. **Mini-batch training without LSTM.** The model does not do as well in an on-line setting. That could be attributed to the LSTM component requiring more training, which depends on the batch. To compare, we test a version of the model without the LSTM component on the same dataset dataset, which is faster and sometimes gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3734d833",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.3419\u001b[0m       \u001b[32m1.0000\u001b[0m        \u001b[35m2.2270\u001b[0m  0.1362\n",
      "      2        \u001b[36m2.0523\u001b[0m       0.0000        2.2424  0.1517\n",
      "      3        2.4576       1.0000        \u001b[35m2.2160\u001b[0m  0.1398\n",
      "      4        2.2452       0.0000        2.2598  0.1262\n",
      "      5        2.4985       0.0000        2.2694  0.1393\n",
      "      6        2.9719       0.0000        2.3904  0.1710\n",
      "      7        2.6679       0.0000        2.3867  0.2245\n",
      "      8        2.5224       0.0000        2.3298  0.1556\n",
      "      9        2.2429       1.0000        \u001b[35m2.2023\u001b[0m  0.1264\n",
      "     10        2.5001       0.0000        2.3692  0.1340\n",
      "[7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "[9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0\n",
      " 0 0 0 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8 8\n",
      " 8 8 8 8 8 8 7 7 7 7 7 7 7 7 7 7 1 1 1 1 1 1 1 1 1 1]\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "model_skorch = FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "    \n",
    "# Batch processing, we have 100 time series samples, so the model trains with 10 examples every time\n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43969c36",
   "metadata": {},
   "source": [
    "6. **Loading a larger dataset.** To test this more, we can try the two incremental versions of the model on a larger time-series dataset, such as [FordA](http://timeseriesclassification.com/description.php?Dataset=FordA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594d98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "r = requests.get(\"http://timeseriesclassification.com/Downloads/FordA.zip\", stream=True)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc165467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3601, 500)\n",
      "(3601,)\n",
      "[[-0.79717165 -0.66439205 -0.37301463 ... -0.66439205 -1.0737958\n",
      "  -1.5643427 ]\n",
      " [ 0.8048547   0.6346286   0.37347448 ... -0.71488506 -0.5604429\n",
      "  -0.31908643]\n",
      " [ 0.7279851   0.11128392 -0.49912438 ...  0.39446303  0.3394004\n",
      "   0.2553906 ]\n",
      " ...\n",
      " [-0.5700543  -0.33316523 -0.29351854 ... -1.3937145  -0.9427333\n",
      "  -0.27072167]\n",
      " [ 2.006732    2.07915     2.0220363  ... -0.43214503 -0.44123125\n",
      "  -0.2807089 ]\n",
      " [-0.1252409  -0.32536268 -0.48823696 ...  0.5557605   0.574451\n",
      "   0.573116  ]]\n",
      "[0 1 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = arff.load(open('data/FordA_TRAIN.arff'))\n",
    "train_data = np.array(train_dataset['data'])\n",
    "\n",
    "X_train = train_data[:,0:-1]\n",
    "y_train = train_data[:,-1]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_train)\n",
    "        \n",
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "        \n",
    "print(y_train)\n",
    "    \n",
    "\n",
    "test_dataset = arff.load(open('data/FordA_TEST.arff'))\n",
    "\n",
    "test_data = np.array(test_dataset['data'])\n",
    "\n",
    "X_test = test_data[:,0:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "y_test = np.where(y_test == -1, 0, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95367912",
   "metadata": {},
   "source": [
    "7. **Mini-batch learning on the larger dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3088a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7030\u001b[0m       \u001b[32m0.4500\u001b[0m        \u001b[35m0.6958\u001b[0m  2.6309\n",
      "      2        \u001b[36m0.6998\u001b[0m       0.4500        \u001b[35m0.6957\u001b[0m  2.0366\n",
      "      3        \u001b[36m0.6979\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.6933\u001b[0m  2.1787\n",
      "      4        \u001b[36m0.6971\u001b[0m       0.5000        \u001b[35m0.6932\u001b[0m  1.9748\n",
      "      5        0.7001       0.5000        \u001b[35m0.6927\u001b[0m  1.9154\n",
      "      6        \u001b[36m0.6848\u001b[0m       0.4500        0.6932  1.8258\n",
      "      7        0.6893       \u001b[32m0.6000\u001b[0m        \u001b[35m0.6922\u001b[0m  2.0832\n",
      "      8        \u001b[36m0.6832\u001b[0m       0.5500        0.6933  2.1145\n",
      "      9        \u001b[36m0.6760\u001b[0m       0.6000        \u001b[35m0.6916\u001b[0m  2.1590\n",
      "     10        \u001b[36m0.6712\u001b[0m       0.5000        0.6925  2.0586\n",
      "     11        \u001b[36m0.6649\u001b[0m       \u001b[32m0.7000\u001b[0m        \u001b[35m0.6901\u001b[0m  2.0862\n",
      "     12        0.6683       0.5500        0.6919  2.3860\n",
      "     13        \u001b[36m0.6618\u001b[0m       0.6000        0.6916  2.6867\n",
      "     14        \u001b[36m0.6579\u001b[0m       \u001b[32m0.7500\u001b[0m        \u001b[35m0.6881\u001b[0m  2.6195\n",
      "     15        0.6603       0.6000        0.6899  2.4152\n",
      "     16        \u001b[36m0.6484\u001b[0m       0.6000        0.6917  2.2890\n",
      "     17        0.6498       0.6000        0.6894  2.1359\n",
      "     18        \u001b[36m0.6396\u001b[0m       0.6000        0.6888  2.6641\n",
      "     19        0.6412       0.5500        0.6912  2.2218\n",
      "     20        0.6620       0.6500        \u001b[35m0.6875\u001b[0m  1.9230\n",
      "     21        0.6589       0.6500        \u001b[35m0.6822\u001b[0m  1.9069\n",
      "     22        0.6484       0.7500        0.6831  2.2661\n",
      "     23        \u001b[36m0.6311\u001b[0m       0.7500        \u001b[35m0.6770\u001b[0m  2.0412\n",
      "     24        \u001b[36m0.6121\u001b[0m       \u001b[32m0.8500\u001b[0m        \u001b[35m0.6674\u001b[0m  1.9737\n",
      "     25        0.6224       0.6500        0.6857  1.9318\n",
      "     26        0.6151       0.7500        0.6739  2.0356\n",
      "     27        0.6377       0.6500        0.6780  1.9896\n",
      "     28        0.6489       0.7500        \u001b[35m0.6591\u001b[0m  2.4662\n",
      "     29        0.6326       0.7500        \u001b[35m0.6575\u001b[0m  1.9968\n",
      "     30        0.6430       0.5000        0.6964  2.0707\n",
      "     31        0.6150       0.7000        0.6672  2.3575\n",
      "     32        0.6320       0.8000        \u001b[35m0.6549\u001b[0m  2.1329\n",
      "     33        0.6354       0.8000        \u001b[35m0.6414\u001b[0m  2.1682\n",
      "     34        0.6201       0.6500        0.6665  2.0606\n",
      "     35        0.6286       0.8500        \u001b[35m0.6329\u001b[0m  1.9806\n",
      "     36        0.6182       0.6667        0.6480  1.9799\n",
      "[0 0 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.703030303030303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import gen_batches\n",
    "\n",
    "batch_size = 100\n",
    "classes = 2\n",
    "\n",
    "model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=500, lstm_layers=8, classes=classes)\n",
    "\n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    y_train_batch = np.where(y_train_batch == -1, 0, y_train_batch)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8c284",
   "metadata": {},
   "source": [
    "8. **Mini-batch learning on the larger dataset without LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe611ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7002\u001b[0m       \u001b[32m0.4500\u001b[0m        \u001b[35m0.6971\u001b[0m  0.5402\n",
      "      2        \u001b[36m0.6939\u001b[0m       0.4500        0.6972  0.5470\n",
      "      3        \u001b[36m0.6900\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.6935\u001b[0m  0.5213\n",
      "      4        \u001b[36m0.6822\u001b[0m       0.5000        \u001b[35m0.6933\u001b[0m  0.5610\n",
      "      5        \u001b[36m0.6699\u001b[0m       0.5000        \u001b[35m0.6931\u001b[0m  0.5475\n",
      "      6        \u001b[36m0.6634\u001b[0m       0.4500        0.6958  0.5646\n",
      "      7        0.6718       \u001b[32m0.5500\u001b[0m        \u001b[35m0.6888\u001b[0m  0.4333\n",
      "      8        0.6684       0.4500        0.6963  0.4213\n",
      "      9        \u001b[36m0.6546\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m0.6840\u001b[0m  0.4056\n",
      "     10        \u001b[36m0.6412\u001b[0m       0.5000        0.6927  0.4186\n",
      "     11        0.6436       0.5000        0.6896  0.4066\n",
      "     12        0.6429       0.5000        0.6906  0.4211\n",
      "     13        \u001b[36m0.6213\u001b[0m       0.4500        0.6943  0.4150\n",
      "     14        0.6334       0.5500        \u001b[35m0.6812\u001b[0m  0.4192\n",
      "     15        0.6422       0.4500        0.6933  0.4244\n",
      "     16        \u001b[36m0.6098\u001b[0m       0.4500        0.6949  0.4047\n",
      "     17        0.6291       0.4500        0.6928  0.4135\n",
      "     18        0.6189       0.4500        0.6906  0.4221\n",
      "     19        0.6144       0.5000        0.6935  0.4020\n",
      "     20        0.6454       0.6000        \u001b[35m0.6794\u001b[0m  0.4103\n",
      "     21        0.6386       0.5500        \u001b[35m0.6764\u001b[0m  0.4154\n",
      "     22        0.6278       0.5500        0.6789  0.4095\n",
      "     23        \u001b[36m0.6067\u001b[0m       0.6000        \u001b[35m0.6710\u001b[0m  0.4359\n",
      "     24        \u001b[36m0.5706\u001b[0m       \u001b[32m0.7000\u001b[0m        \u001b[35m0.6540\u001b[0m  0.4306\n",
      "     25        0.5904       0.5000        0.6869  0.4356\n",
      "     26        0.5728       0.6500        0.6601  0.4090\n",
      "     27        0.6117       0.6500        0.6704  0.4178\n",
      "     28        0.6194       \u001b[32m0.7500\u001b[0m        \u001b[35m0.6386\u001b[0m  0.4075\n",
      "     29        0.6036       0.7000        0.6450  0.4111\n",
      "     30        0.6233       0.5000        0.7046  0.4137\n",
      "     31        0.5967       0.6500        0.6618  0.4129\n",
      "     32        0.5994       0.7000        0.6445  0.4127\n",
      "     33        0.6048       \u001b[32m0.8000\u001b[0m        \u001b[35m0.6092\u001b[0m  0.4239\n",
      "     34        0.5874       0.6500        0.6526  0.4072\n",
      "     35        0.6019       0.8000        \u001b[35m0.6050\u001b[0m  0.4069\n",
      "     36        0.5927       0.6667        0.6398  0.4155\n",
      "[0 0 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.6984848484848485\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "classes = 2\n",
    "\n",
    "#model_skorch = LSTM_FCN_Classifier(in_channels=1,input_size=1460, lstm_layers=8, classes=classes)\n",
    "model_skorch = FCN_Classifier(in_channels=1,input_size=945, lstm_layers=8, classes=classes)\n",
    "    \n",
    "for batch in gen_batches(train_data.shape[0], batch_size, min_batch_size=batch_size):\n",
    "    current_batch = train_data[batch]\n",
    "    \n",
    "    X_train_batch = current_batch[:,0:-1]\n",
    "    y_train_batch = current_batch[:,-1]\n",
    "\n",
    "    X_train_batch = X_train_batch.astype(np.float32)\n",
    "    y_train_batch = y_train_batch.astype(np.int64)\n",
    "    \n",
    "    y_train_batch = np.where(y_train_batch == -1, 0, y_train_batch)\n",
    "    \n",
    "    partial_fit = model_skorch.partial_fit(X_train_batch, y_train_batch)\n",
    "\n",
    "predict = model_skorch.predict(X_test)\n",
    "\n",
    "print(predict)\n",
    "print(y_test)\n",
    "\n",
    "print(partial_fit.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
