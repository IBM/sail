{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"InceptionTime.ipynb","provenance":[],"authorship_tag":"ABX9TyOF8JT6xpITNRmcL+IM9TAI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["This notebook shows the implementation of a skorch wrapper class containing the InceptionTime model. Code for the model itself was based on https://github.com/okrasolar/pytorch-timeseries\n","\n","Author: Christos C. Papadopoulos\n","https://github.com/Christosc96"],"metadata":{"id":"4aWn-DaOwR9V"}},{"cell_type":"code","source":["!pip install skorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"666TNE3dvIHG","executionInfo":{"status":"ok","timestamp":1653691700172,"user_tz":-180,"elapsed":2967,"user":{"displayName":"Christos Papadopoulos","userId":"08141375972921064188"}},"outputId":"7d62052b-ec2e-486a-a888-fdfdc06b5f63"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting skorch\n","  Downloading skorch-0.11.0-py3-none-any.whl (155 kB)\n","\u001b[K     |████████████████████████████████| 155 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.64.0)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.0.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.4.1)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.9)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.21.6)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch) (1.1.0)\n","Installing collected packages: skorch\n","Successfully installed skorch-0.11.0\n"]}]},{"cell_type":"code","source":["\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","from sklearn.datasets import make_classification\n","from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","\n","\n","from typing import cast, Union, List\n","from skorch import NeuralNetClassifier"],"metadata":{"id":"mwZxjz7lvNS_","executionInfo":{"status":"ok","timestamp":1653691702245,"user_tz":-180,"elapsed":501,"user":{"displayName":"Christos Papadopoulos","userId":"08141375972921064188"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["'''\n","Define conv1d with same padding\n","'''\n","\n","\n","\n","class Conv1dSamePadding(nn.Conv1d):\n","    \"\"\"Represents the \"Same\" padding functionality from Tensorflow.\n","    See: https://github.com/pytorch/pytorch/issues/3867\n","    Note that the padding argument in the initializer doesn't do anything now\n","    \"\"\"\n","    def forward(self, input):\n","        return conv1d_same_padding(input, self.weight, self.bias, self.stride,\n","                                   self.dilation, self.groups)\n","\n","\n","def conv1d_same_padding(input, weight, bias, stride, dilation, groups):\n","    # stride and dilation are expected to be tuples.\n","    kernel, dilation, stride = weight.size(2), dilation[0], stride[0]\n","    l_out = l_in = input.size(2)\n","    padding = (((l_out - 1) * stride) - l_in + (dilation * (kernel - 1)) + 1)\n","    if padding % 2 != 0:\n","        input = F.pad(input, [0, 1])\n","\n","    return F.conv1d(input=input, weight=weight, bias=bias, stride=stride,\n","                    padding=padding // 2,\n","                    dilation=dilation, groups=groups)\n","\n","\n","class ConvBlock(nn.Module):\n","\n","    def __init__(self, in_channels: int, out_channels: int, kernel_size: int,\n","                 stride: int) -> None:\n","        super().__init__()\n","\n","        self.layers = nn.Sequential(\n","            Conv1dSamePadding(in_channels=in_channels,\n","                              out_channels=out_channels,\n","                              kernel_size=kernel_size,\n","                              stride=stride),\n","            nn.BatchNorm1d(num_features=out_channels),\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore\n","\n","        return self.layers(x)"],"metadata":{"id":"BexVwE0lxxBw","executionInfo":{"status":"ok","timestamp":1653691705256,"user_tz":-180,"elapsed":215,"user":{"displayName":"Christos Papadopoulos","userId":"08141375972921064188"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Define the _InceptionModel class. Code is based on https://github.com/okrasolar/pytorch-timeseries"],"metadata":{"id":"GKaoP2_BFwIL"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"QwPIUQoDxGiR","executionInfo":{"status":"ok","timestamp":1653691706909,"user_tz":-180,"elapsed":476,"user":{"displayName":"Christos Papadopoulos","userId":"08141375972921064188"}}},"outputs":[],"source":["class _InceptionModel(nn.Module):\n","    \"\"\"A PyTorch implementation of the InceptionTime model.\n","    From https://arxiv.org/abs/1909.04939\n","    Attributes\n","    ----------\n","    num_blocks:\n","        The number of inception blocks to use. One inception block consists\n","        of 3 convolutional layers, (optionally) a bottleneck and (optionally) a residual\n","        connector\n","    in_channels:\n","        The number of input channels (i.e. input.shape[-1])\n","    out_channels:\n","        The number of \"hidden channels\" to use. Can be a list (for each block) or an\n","        int, in which case the same value will be applied to each block\n","    bottleneck_channels:\n","        The number of channels to use for the bottleneck. Can be list or int. If 0, no\n","        bottleneck is applied\n","    kernel_sizes:\n","        The size of the kernels to use for each inception block. Within each block, each\n","        of the 3 convolutional layers will have kernel size\n","        `[kernel_size // (2 ** i) for i in range(3)]`\n","    num_pred_classes:\n","        The number of output classes\n","    \"\"\"\n","\n","    def __init__(self, num_blocks: int, in_channels: int, out_channels: Union[List[int], int],\n","                 bottleneck_channels: Union[List[int], int], kernel_sizes: Union[List[int], int],\n","                 use_residuals: Union[List[bool], bool, str] = 'default',\n","                 num_pred_classes: int = 1\n","                 ) -> None:\n","        super().__init__()\n","\n","        # for easier saving and loading\n","        self.input_args = {\n","            'num_blocks': num_blocks,\n","            'in_channels': in_channels,\n","            'out_channels': out_channels,\n","            'bottleneck_channels': bottleneck_channels,\n","            'kernel_sizes': kernel_sizes,\n","            'use_residuals': use_residuals,\n","            'num_pred_classes': num_pred_classes\n","        }\n","\n","        channels = [in_channels] + cast(List[int], self._expand_to_blocks(out_channels,\n","                                                                          num_blocks))\n","        bottleneck_channels = cast(List[int], self._expand_to_blocks(bottleneck_channels,\n","                                                                     num_blocks))\n","        kernel_sizes = cast(List[int], self._expand_to_blocks(kernel_sizes, num_blocks))\n","        if use_residuals == 'default':\n","            use_residuals = [True if i % 3 == 2 else False for i in range(num_blocks)]\n","        use_residuals = cast(List[bool], self._expand_to_blocks(\n","            cast(Union[bool, List[bool]], use_residuals), num_blocks)\n","        )\n","\n","        self.blocks = nn.Sequential(*[\n","            _InceptionBlock(in_channels=channels[i], out_channels=channels[i + 1],\n","                           residual=use_residuals[i], bottleneck_channels=bottleneck_channels[i],\n","                           kernel_size=kernel_sizes[i]) for i in range(num_blocks)\n","        ])\n","\n","        # a global average pooling (i.e. mean of the time dimension) is why\n","        # in_features=channels[-1]\n","        self.linear = nn.Linear(in_features=channels[-1], out_features=num_pred_classes)\n","\n","    @staticmethod\n","    def _expand_to_blocks(value: Union[int, bool, List[int], List[bool]],\n","                          num_blocks: int) -> Union[List[int], List[bool]]:\n","        if isinstance(value, list):\n","            assert len(value) == num_blocks, \\\n","                f'Length of inputs lists must be the same as num blocks, ' \\\n","                f'expected length {num_blocks}, got {len(value)}'\n","        else:\n","            value = [value] * num_blocks\n","        return value\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore\n","        x = self.blocks(x).mean(dim=-1)  # the mean is the global average pooling\n","        return self.linear(x)\n","\n","\n","class _InceptionBlock(nn.Module):\n","    \"\"\"An inception block consists of an (optional) bottleneck, followed\n","    by 3 conv1d layers. Optionally residual\n","    \"\"\"\n","\n","    def __init__(self, in_channels: int, out_channels: int,\n","                 residual: bool, stride: int = 1, bottleneck_channels: int = 32,\n","                 kernel_size: int = 41) -> None:\n","        assert kernel_size > 3, \"Kernel size must be strictly greater than 3\"\n","        super().__init__()\n","\n","        self.use_bottleneck = bottleneck_channels > 0\n","        if self.use_bottleneck:\n","            self.bottleneck = Conv1dSamePadding(in_channels, bottleneck_channels,\n","                                                kernel_size=1, bias=False)\n","        kernel_size_s = [kernel_size // (2 ** i) for i in range(3)]\n","        start_channels = bottleneck_channels if self.use_bottleneck else in_channels\n","        channels = [start_channels] + [out_channels] * 3\n","        self.conv_layers = nn.Sequential(*[\n","            Conv1dSamePadding(in_channels=channels[i], out_channels=channels[i + 1],\n","                              kernel_size=kernel_size_s[i], stride=stride, bias=False)\n","            for i in range(len(kernel_size_s))\n","        ])\n","\n","        self.batchnorm = nn.BatchNorm1d(num_features=channels[-1])\n","        self.relu = nn.ReLU()\n","\n","        self.use_residual = residual\n","        if residual:\n","            self.residual = nn.Sequential(*[\n","                Conv1dSamePadding(in_channels=in_channels, out_channels=out_channels,\n","                                  kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm1d(out_channels),\n","                nn.ReLU()\n","            ])\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore\n","        org_x = x\n","        if self.use_bottleneck:\n","            x = self.bottleneck(x)\n","        x = self.conv_layers(x)\n","\n","        if self.use_residual:\n","            x = x + self.residual(org_x)\n","        return x"]},{"cell_type":"code","source":["'''\n","wrapper scorch class\n","'''\n","\n","\n","class InceptionTimeClassifier(NeuralNetClassifier):\n","  def __init__(self, num_blocks=2, in_channels=1, out_channels=2,\n","                           bottleneck_channels=2, kernel_sizes=41, use_residuals=True,\n","                           num_pred_classes=1, learning_rate=0.05, batch_size=1000, criterion = nn.BCEWithLogitsLoss, max_epochs = 50):\n","    \n","    self.inception_model = _InceptionModel(num_blocks=2, in_channels=1, out_channels=2,\n","                           bottleneck_channels=2, kernel_sizes=41, use_residuals=True,\n","                           num_pred_classes=1)\n","    \n","    super(InceptionTimeClassifier, self).__init__(\n","            module=self.inception_model,\n","            max_epochs=max_epochs,\n","            lr=learning_rate,\n","            batch_size=batch_size,\n","            criterion=criterion ,\n","            # Shuffle training data on each epoch\n","            iterator_train__shuffle=False,\n","            device='cuda',\n","            )"],"metadata":{"id":"tpTQ1NKnyzOu","executionInfo":{"status":"ok","timestamp":1653691710182,"user_tz":-180,"elapsed":396,"user":{"displayName":"Christos Papadopoulos","userId":"08141375972921064188"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Generate data for training the model. The data used originates from the FordA dataset which is included in the InceptionTime paper as well"],"metadata":{"id":"TD7Lf8xSFIWk"}},{"cell_type":"code","source":["def readucr(filename):\n","    data = np.loadtxt(filename, delimiter=\"\\t\")\n","    y = data[:, 0]\n","    x = data[:, 1:]\n","    return x, y.astype(int)\n","\n","\n","root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n","\n","x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n","x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n","\n","x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1], ))\n","x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n","y_train[y_train == -1] = 0\n","y_test[y_test == -1] = 0\n","x_train = torch.from_numpy(x_train)\n","y_train = torch.from_numpy(y_train)\n","y_train=y_train.unsqueeze(1)\n","\n","x_train=x_train.to(torch.float32)\n","y_train=y_train.to(torch.float32)"],"metadata":{"id":"vjUwoPR2c9JJ","executionInfo":{"status":"ok","timestamp":1653691715847,"user_tz":-180,"elapsed":2644,"user":{"displayName":"Christos Papadopoulos","userId":"08141375972921064188"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["To train the model, since we are using a Scorch wrapper, simply call fit after creating it. "],"metadata":{"id":"Jh9x3fsTFaUR"}},{"cell_type":"code","source":["model = InceptionTimeClassifier(out_channels=10, bottleneck_channels=10, batch_size=500, max_epochs=20, learning_rate=0.5)\n","model.fit(x_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeerDclv6B5D","executionInfo":{"status":"ok","timestamp":1653691950777,"user_tz":-180,"elapsed":1912,"user":{"displayName":"Christos Papadopoulos","userId":"08141375972921064188"}},"outputId":"0a15b5fb-8ca0-490d-baf6-35e3d4f7f878"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m0.6988\u001b[0m       \u001b[32m0.4868\u001b[0m        \u001b[35m0.6935\u001b[0m  0.1272\n","      2        \u001b[36m0.6930\u001b[0m       \u001b[32m0.5132\u001b[0m        \u001b[35m0.6927\u001b[0m  0.1280\n","      3        \u001b[36m0.6921\u001b[0m       0.5132        \u001b[35m0.6926\u001b[0m  0.1027\n","      4        \u001b[36m0.6904\u001b[0m       0.5132        \u001b[35m0.6922\u001b[0m  0.0912\n","      5        \u001b[36m0.6824\u001b[0m       0.5132        \u001b[35m0.6902\u001b[0m  0.0862\n","      6        \u001b[36m0.6707\u001b[0m       \u001b[32m0.5520\u001b[0m        \u001b[35m0.6787\u001b[0m  0.0742\n","      7        \u001b[36m0.6656\u001b[0m       \u001b[32m0.6491\u001b[0m        \u001b[35m0.6657\u001b[0m  0.0755\n","      8        \u001b[36m0.6593\u001b[0m       \u001b[32m0.6602\u001b[0m        \u001b[35m0.6566\u001b[0m  0.0759\n","      9        \u001b[36m0.6535\u001b[0m       0.6519        \u001b[35m0.6483\u001b[0m  0.0713\n","     10        \u001b[36m0.6462\u001b[0m       \u001b[32m0.6727\u001b[0m        \u001b[35m0.6389\u001b[0m  0.0725\n","     11        \u001b[36m0.6353\u001b[0m       \u001b[32m0.6935\u001b[0m        \u001b[35m0.6223\u001b[0m  0.0743\n","     12        \u001b[36m0.6143\u001b[0m       \u001b[32m0.7850\u001b[0m        \u001b[35m0.5871\u001b[0m  0.0754\n","     13        \u001b[36m0.5859\u001b[0m       \u001b[32m0.8946\u001b[0m        \u001b[35m0.5399\u001b[0m  0.0735\n","     14        \u001b[36m0.5575\u001b[0m       0.8932        \u001b[35m0.5128\u001b[0m  0.0742\n","     15        \u001b[36m0.5301\u001b[0m       \u001b[32m0.9001\u001b[0m        \u001b[35m0.4859\u001b[0m  0.0783\n","     16        \u001b[36m0.5040\u001b[0m       0.8974        \u001b[35m0.4622\u001b[0m  0.0745\n","     17        \u001b[36m0.4795\u001b[0m       0.8960        \u001b[35m0.4380\u001b[0m  0.0773\n","     18        \u001b[36m0.4533\u001b[0m       0.9001        \u001b[35m0.4174\u001b[0m  0.0727\n","     19        \u001b[36m0.4285\u001b[0m       0.9001        \u001b[35m0.4012\u001b[0m  0.0761\n","     20        \u001b[36m0.4066\u001b[0m       0.9001        \u001b[35m0.3863\u001b[0m  0.0744\n"]},{"output_type":"execute_result","data":{"text/plain":["<class '__main__.InceptionTimeClassifier'>[initialized](\n","  module_=_InceptionModel(\n","    (blocks): Sequential(\n","      (0): _InceptionBlock(\n","        (bottleneck): Conv1dSamePadding(1, 2, kernel_size=(1,), stride=(1,), bias=False)\n","        (conv_layers): Sequential(\n","          (0): Conv1dSamePadding(2, 2, kernel_size=(41,), stride=(1,), bias=False)\n","          (1): Conv1dSamePadding(2, 2, kernel_size=(20,), stride=(1,), bias=False)\n","          (2): Conv1dSamePadding(2, 2, kernel_size=(10,), stride=(1,), bias=False)\n","        )\n","        (batchnorm): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","        (residual): Sequential(\n","          (0): Conv1dSamePadding(1, 2, kernel_size=(1,), stride=(1,), bias=False)\n","          (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU()\n","        )\n","      )\n","      (1): _InceptionBlock(\n","        (bottleneck): Conv1dSamePadding(2, 2, kernel_size=(1,), stride=(1,), bias=False)\n","        (conv_layers): Sequential(\n","          (0): Conv1dSamePadding(2, 2, kernel_size=(41,), stride=(1,), bias=False)\n","          (1): Conv1dSamePadding(2, 2, kernel_size=(20,), stride=(1,), bias=False)\n","          (2): Conv1dSamePadding(2, 2, kernel_size=(10,), stride=(1,), bias=False)\n","        )\n","        (batchnorm): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU()\n","        (residual): Sequential(\n","          (0): Conv1dSamePadding(2, 2, kernel_size=(1,), stride=(1,), bias=False)\n","          (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU()\n","        )\n","      )\n","    )\n","    (linear): Linear(in_features=2, out_features=1, bias=True)\n","  ),\n",")"]},"metadata":{},"execution_count":18}]}]}