"""
Time series classification
Implementation of Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline (2016, arXiv) in Keras by using a scikeras wrapper 
This is just an experiment to implement all the examined models of the paper!!!

Authors: Hao Miao
"""
 
from __future__ import print_function
 
from tensorflow import keras
from tr_classification_model import *
from scikeras.wrappers import KerasClassifier
import numpy as np
import pandas as pd

# def readucr(filename):
#     data = np.loadtxt(filename, delimiter = ',')
#     Y = data[:,0]
#     X = data[:,1:]
#     return X, Y
  
nb_epochs = 5


#flist = ['Adiac', 'Beef', 'CBF', 'ChlorineConcentration', 'CinC_ECG_torso', 'Coffee', 'Cricket_X', 'Cricket_Y', 'Cricket_Z', 
#'DiatomSizeReduction', 'ECGFiveDays', 'FaceAll', 'FaceFour', 'FacesUCR', '50words', 'FISH', 'Gun_Point', 'Haptics', 
#'InlineSkate', 'ItalyPowerDemand', 'Lighting2', 'Lighting7', 'MALLAT', 'MedicalImages', 'MoteStrain', 'NonInvasiveFatalECG_Thorax1', 
#'NonInvasiveFatalECG_Thorax2', 'OliveOil', 'OSULeaf', 'SonyAIBORobotSurface', 'SonyAIBORobotSurfaceII', 'StarLightCurves', 'SwedishLeaf', 'Symbols', 
#'synthetic_control', 'Trace', 'TwoLeadECG', 'Two_Patterns', 'uWaveGestureLibrary_X', 'uWaveGestureLibrary_Y', 'uWaveGestureLibrary_Z', 'wafer', 'WordsSynonyms', 'yoga']

flist  = ['Adiac']
for each in flist:
    fname = each
    x_train, y_train = readucr(fname+'_TRAIN')
    x_test, y_test = readucr(fname+'_TEST')
    nb_classes = len(np.unique(y_test))
    batch_size = min(x_train.shape[0]/10, 16)
    
    y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)
    y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)
    
    
    Y_train = keras.utils.to_categorical(y_train, nb_classes)
    Y_test = keras.utils.to_categorical(y_test, nb_classes)
    
    x_train_mean = x_train.mean()
    x_train_std = x_train.std()
    x_train = (x_train - x_train_mean)/(x_train_std)
     
    x_test = (x_test - x_train_mean)/(x_train_std)
    x_train = x_train.reshape(x_train.shape + (1,1,))
    x_test = x_test.reshape(x_test.shape + (1,1,))

#     x = keras.layers.Input(x_train.shape[1:])
# #    drop_out = Dropout(0.2)(x)
#     conv1 = keras.layers.Conv2D(128, 8, 1, padding='same')(x)
#     conv1 = keras.layers.BatchNormalization()(conv1)
#     conv1 = keras.layers.Activation('relu')(conv1)
    
# #    drop_out = Dropout(0.2)(conv1)
#     conv2 = keras.layers.Conv2D(256, 5, 1, padding='same')(conv1)
#     conv2 = keras.layers.BatchNormalization()(conv2)
#     conv2 = keras.layers.Activation('relu')(conv2)
    
# #    drop_out = Dropout(0.2)(conv2)
#     conv3 = keras.layers.Conv2D(128, 3, 1, padding='same')(conv2)
#     conv3 = keras.layers.BatchNormalization()(conv3)
#     conv3 = keras.layers.Activation('relu')(conv3)
    
#     full = keras.layers.GlobalAveragePooling2D()(conv3)
#     out = keras.layers.Dense(nb_classes, activation='softmax')(full)
    
    
#     model = keras.models.Model(inputs=x, outputs=out)
    # FCN
    model = get_FCN_model(x_train, nb_classes)
    clf = KerasClassifier(model, loss='categorical_crossentropy', epochs=2000)
    clf.fit(x_train, Y_train)
    # y_proba = clf.predict_proba()
    print(clf.score(x_test, Y_test))
    
    # MLP
    model = get_MLP_model(x_train, nb_classes)
    clf = KerasClassifier(model, loss='categorical_crossentropy', epochs=2000)
    clf.fit(x_train, Y_train)
    # y_proba = clf.predict_proba()
    print(clf.score(x_test, Y_test))

    # ResNet
    model = get_ResNet_model(x_train, nb_classes)
    clf = KerasClassifier(model, loss='categorical_crossentropy', epochs=2000)
    clf.fit(x_train, Y_train)
    # y_proba = clf.predict_proba()
    print(clf.score(x_test, Y_test))

     
    # optimizer = keras.optimizers.Adam()
    # model.compile(loss='categorical_crossentropy',
    #               optimizer=optimizer,
    #               metrics=['accuracy'])
     
    # reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'loss', factor=0.5,
    #                   patience=50, min_lr=0.0001) 
    # hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs,
    #           verbose=1, validation_data=(x_test, Y_test), callbacks = [reduce_lr])
    # #Print the testing results which has the lowest training loss.
    # log = pd.DataFrame(hist.history)
    # print(log.loc[log['loss'].idxmin]['loss'], log.loc[log['loss'].idxmin]['val_acc'])


############## Get CAM ################
# import matplotlib.pyplot as plt
# # from matplotlib.backends.backend_pdf import PdfPages

# get_last_conv = keras.backend.function([model.layers[0].input, keras.backend.learning_phase()], [model.layers[-3].output])
# last_conv = get_last_conv([x_test[:100], 1])[0]

# get_softmax = keras.backend.function([model.layers[0].input, keras.backend.learning_phase()], [model.layers[-1].output])
# softmax = get_softmax(([x_test[:100], 1]))[0]
# softmax_weight = model.get_weights()[-2]
# CAM = np.dot(last_conv, softmax_weight)


# # pp = PdfPages('CAM.pdf')
# for k in range(20):
#     CAM = (CAM - CAM.min(axis=1, keepdims=True)) / (CAM.max(axis=1, keepdims=True) - CAM.min(axis=1, keepdims=True))
#     c = np.exp(CAM) / np.sum(np.exp(CAM), axis=1, keepdims=True)
#     plt.figure(figsize=(13, 7));
#     plt.plot(x_test[k].squeeze());
#     plt.scatter(np.arange(len(x_test[k])), x_test[k].squeeze(), cmap='hot_r', c=c[k, :, :, int(y_test[k])].squeeze(), s=100);
#     plt.title(
#         'True label:' + str(y_test[k]) + '   likelihood of label ' + str(y_test[k]) + ': ' + str(softmax[k][int(y_test[k])]))
#     plt.colorbar();
#     pp.savefig()
#
# pp.close()