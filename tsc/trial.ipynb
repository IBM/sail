{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series classification\n",
    "Implementation of Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline (2016, arXiv) in PyTorch by using a skorch wrapper \n",
    "In this script, we are using a dataset that was originally used in the paper\n",
    "\n",
    "Authors: Marina Georgati, Hao Miao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NM12LQ\\Anaconda3\\envs\\imla\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import our modules\n",
    "import numpy as np\n",
    "from src import model, utils, tsc\n",
    "# Import SKORCH NN classifier\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Adiac:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory1: c:\\Users\\NM12LQ\\OneDrive - Aalborg Universitet\\PhD\\PhDCourses\\11. IMLA\\Marina\n",
      "Current working directory1: c:\\Users\\NM12LQ\\OneDrive - Aalborg Universitet\\PhD\\PhDCourses\\11. IMLA\\Marina\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Adiac: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# The dataset is downloaded by http://www.timeseriesclassification.com/Downloads/\n",
    "datasets = ['Adiac']\n",
    "# utils.download_datasets(datasets)  # uncomment this to download the data\n",
    "# A dictionary is created by train/test DataLoaders for the downloaded dataset\n",
    "dataset_dictionary = utils.data_dictionary(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training dataset is: (390, 176)\n",
      "The shape of the test dataset is: (391, 176)\n",
      "Number of input units: 176\n",
      "Number of classes: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NM12LQ\\Anaconda3\\envs\\imla\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m4.1683\u001b[0m       \u001b[32m0.0769\u001b[0m        \u001b[35m3.5628\u001b[0m  7.0387\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      2        \u001b[36m3.4040\u001b[0m       \u001b[32m0.1026\u001b[0m        \u001b[35m3.2466\u001b[0m  7.4042\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      3        \u001b[36m3.3383\u001b[0m       0.1026        \u001b[35m3.2305\u001b[0m  8.1968\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      4        \u001b[36m3.3023\u001b[0m       0.0641        3.2447  8.2408\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      5        \u001b[36m3.2721\u001b[0m       0.1026        \u001b[35m3.2047\u001b[0m  8.9846\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      6        \u001b[36m3.2374\u001b[0m       0.0897        \u001b[35m3.1704\u001b[0m  9.4716\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      7        \u001b[36m3.1823\u001b[0m       0.1026        3.1716  7.7651\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      8        \u001b[36m3.1391\u001b[0m       \u001b[32m0.1282\u001b[0m        \u001b[35m3.1464\u001b[0m  7.4569\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      9        \u001b[36m3.0935\u001b[0m       0.1282        \u001b[35m3.1350\u001b[0m  7.9045\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "     10        \u001b[36m3.0461\u001b[0m       0.1026        \u001b[35m3.0410\u001b[0m  8.0129\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Score: 0.13554987212276215\n"
     ]
    }
   ],
   "source": [
    "for dataset, dataloader in dataset_dictionary.items():\n",
    "    # For using the downloaded dataset, a dataset and a dataloader are created. \n",
    "    # Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples\n",
    "    print('The shape of the training dataset is:', dataloader['train'].dataset.x.shape)\n",
    "    print('The shape of the test dataset is:', dataloader['test'].dataset.x.shape)\n",
    "    \n",
    "    time_steps = dataloader['test'].dataset.x.shape[-1] # The Number of input units\n",
    "    n_classes  = len(np.unique(dataloader['test'].dataset.y)) # Number of classes\n",
    "    print('Number of input units:', time_steps)\n",
    "    print('Number of classes:', n_classes)\n",
    "    \n",
    "    # The Neural Net is initialized with fixed hyperparameters\n",
    "    nn = NeuralNetClassifier(\n",
    "        model._ConvNetModel(time_steps, n_classes), \n",
    "        max_epochs=10, \n",
    "        lr=0.1, \n",
    "        batch_size=12, \n",
    "        optimizer=torch.optim.Adam,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        # Shuffle training data on each epoch\n",
    "        iterator_train__shuffle=False )\n",
    "\n",
    "    # The training set is defined here, modification of the datatype might be required\n",
    "    y_train = dataloader['train'].dataset.y\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "    \n",
    "    # The testing set is defined here, modification of the datatype might be required\n",
    "    y_test = dataloader['test'].dataset.y\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "    # Fit the model on the training dataset \n",
    "    nn.fit(dataloader['train'].dataset.x, y_train)\n",
    "    # Return probability estimates for samples\n",
    "    nn.predict_proba(dataloader['test'].dataset.x)\n",
    "    # Return class labels for samples in X\n",
    "    nn.predict(dataloader['test'].dataset.x)\n",
    "    # Print score\n",
    "    print('Score:', nn.score(dataloader['test'].dataset.x, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR DIFFERENTLY , calling the wrapper from src.tsc\n",
    "for dataset, dataloader in dataset_dictionary.items():\n",
    "    print(dataloader['train'].dataset.x.shape)\n",
    "    print(dataloader['test'].dataset.x.shape)\n",
    "    \n",
    "    time_steps = dataloader['test'].dataset.x.shape[-1]\n",
    "    n_classes  = len(np.unique(dataloader['test'].dataset.y))\n",
    "    print(time_steps, n_classes)\n",
    "\n",
    "    # The Neural Net is initialized with fixed hyperparameters\n",
    "    nn = tsc.ConvNet(time_steps, n_classes)\n",
    "\n",
    "    y_train = dataloader['train'].dataset.y\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "    \n",
    "    y_test = dataloader['test'].dataset.y\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "    nn.fit(dataloader['train'].dataset.x, y_train)\n",
    "    print(nn.score(dataloader['test'].dataset.x, y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d4520545ca233a18fcbfaf6de76cf00aab1835d3dba167ce44a459e1af2c853"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('imla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
